# Language Support Features - Product Requirements & Prioritization (PRP)

## Executive Summary

The Language Support features represent critical accessibility and learning enhancement tools for the multilingual reading comprehension webapp. Currently implemented as UI stubs, these features would significantly improve the educational value and user experience for children learning Korean through English reading passages.

## Problem Statement

**Current Gap**: The Language Support section exists in the UI but provides no functional value, creating user confusion and missing critical language learning opportunities.

**User Pain Points**:
- Korean learners cannot hear proper pronunciation of new vocabulary
- No visual assistance when struggling with comprehension
- Missing phonetic guidance for Korean pronunciation
- No audio support for different learning styles
- Limited accessibility for diverse learners

## Feature Analysis & Use Cases

### ðŸ”Š **1. Phonetics** (IPA/Korean Pronunciation Guide)

**Primary Use Cases**:
- **Vocabulary Learning**: Click Korean word â†’ Show IPA notation `[ì•ˆë…•] â†’ [annyeong]`
- **Reading Support**: Hover over Korean text to see pronunciation guide
- **Quiz Integration**: Pronunciation-based quiz questions

**Technical Implementation**:
```typescript
interface PhoneticsConfig {
  enabled: boolean;
  displayType: 'IPA' | 'simplified' | 'both';
  trigger: 'click' | 'hover' | 'toggle';
  languages: ['korean', 'english'];
}
```

**Backend Requirements**:
- Korean pronunciation API integration (Google Text-to-Speech or Azure Speech)
- IPA notation database for Korean phonemes
- Pronunciation accuracy scoring (future)

### ðŸŽµ **2. Audio Support** (Text-to-Speech)

**Primary Use Cases**:
- **"Read to Me" Feature**: Play entire passage with proper pronunciation
- **Word-level Audio**: Click any word to hear pronunciation
- **Quiz Accessibility**: Read quiz questions and answers aloud
- **Sentence-level Practice**: Play individual sentences for repetition

**Technical Implementation**:
```typescript
interface AudioConfig {
  enabled: boolean;
  autoPlay: boolean;
  speed: 0.5 | 0.75 | 1.0 | 1.25 | 1.5;
  voice: {
    english: 'neural-voice-en-US';
    korean: 'neural-voice-ko-KR';
  };
  visualHighlight: boolean; // Highlight words as spoken
}
```

**Backend Requirements**:
- Azure Cognitive Services Speech integration
- Audio caching for common vocabulary
- SSML markup for natural speech patterns

### ðŸ‘ï¸ **3. Visual Context** (Comprehension Aids)

**Primary Use Cases**:
- **Quiz Hints**: Highlight relevant text when user requests hint
- **Vocabulary Tooltips**: Rich definitions with visual examples
- **Reading Focus**: Highlight current sentence during audio playback
- **Progress Visualization**: Show reading progression and vocabulary mastery

**Technical Implementation**:
```typescript
interface VisualContextConfig {
  enabled: boolean;
  hintHighlighting: boolean;
  vocabularyTooltips: boolean;
  readingProgress: boolean;
  sentenceHighlight: boolean;
  colorScheme: 'default' | 'high-contrast' | 'dyslexia-friendly';
}
```

**Frontend Features**:
- Text highlighting with CSS animations
- Tooltip overlays with rich content
- Progress bars and mastery indicators
- Accessibility color schemes

### ðŸ“ **4. Romanization** (Korean â†’ Latin Alphabet)

**Primary Use Cases**:
- **Pronunciation Bridge**: Show romanized Korean for beginners
- **Search Support**: Find Korean words using English keyboard
- **Gradual Learning**: Progressively reduce romanization dependency

**Technical Implementation**:
```typescript
interface RomanizationConfig {
  enabled: boolean;
  system: 'revised' | 'mccune-reischauer' | 'simple';
  displayMode: 'always' | 'hover' | 'toggle';
  fadeOut: boolean; // Gradually hide as user progresses
}
```

**Considerations**:
- Educational debate: Some experts discourage romanization dependency
- Gradual phase-out as users advance in Korean proficiency

### ðŸ“š **5. Grammar Hints** (Structure Explanations)

**Primary Use Cases**:
- **Sentence Structure**: Explain Korean grammar patterns
- **Quiz Explanations**: Why certain answers are grammatically correct
- **Progressive Learning**: Introduce grammar concepts gradually
- **Error Analysis**: Explain common mistakes in fill-in-blank questions

**Technical Implementation**:
```typescript
interface GrammarHintsConfig {
  enabled: boolean;
  complexity: 'basic' | 'intermediate' | 'advanced';
  patterns: string[]; // Grammar patterns to highlight
  explanationDepth: 'simple' | 'detailed';
}
```

**Backend Requirements**:
- Korean grammar pattern database
- NLP analysis for sentence structure
- Age-appropriate grammar explanations

## Priority Matrix

### **ðŸš€ Phase 1 (MVP - High Impact, Medium Effort)**

**1. Audio Support** (Priority: Critical)
- **Impact**: Massive improvement to learning experience
- **Complexity**: Medium (Azure Speech Services integration)
- **User Value**: Essential for pronunciation learning
- **Implementation**: 2-3 weeks

**2. Visual Context** (Priority: High)
- **Impact**: Significant quiz and reading improvements
- **Complexity**: Low-Medium (Frontend-focused)
- **User Value**: Immediate comprehension benefits
- **Implementation**: 1-2 weeks

### **ðŸŒŸ Phase 2 (Enhanced Features - Medium Impact, Medium Effort)**

**3. Phonetics** (Priority: Medium-High)
- **Impact**: Valuable for serious language learners
- **Complexity**: Medium (Pronunciation API integration)
- **User Value**: Professional language learning quality
- **Implementation**: 2-3 weeks

### **ðŸ”„ Phase 3 (Optional Features - Lower Priority)**

**4. Romanization** (Priority: Medium)
- **Impact**: Helpful for beginners, controversial in pedagogy
- **Complexity**: Low (Text transformation)
- **User Value**: Temporary learning aid
- **Implementation**: 1 week

**5. Grammar Hints** (Priority: Low-Medium)
- **Impact**: Advanced learners benefit most
- **Complexity**: High (NLP and content creation)
- **User Value**: Advanced feature
- **Implementation**: 4-6 weeks

## End-to-End User Stories with Frontend/Backend Integration

### ðŸŽ¯ **User Story 1: Quiz Hint System** (Visual Context)

**As a** 4th-grade student struggling with a quiz question
**I want to** get a hint that shows me where to find the answer
**So that** I can learn to locate information in the text independently

#### Frontend/Backend Flow:
```typescript
// User clicks "Give me a hint" button on quiz question #2
1. [Frontend] Captures click event with question ID and current story content
   const hintRequest = {
     questionId: "q2",
     questionText: "What did the main character learn?",
     storyContent: currentStory.englishContent,
     userAttempts: 1
   }

2. [Frontend â†’ Backend] POST /api/generate-hint
   â†’ Sends question + story content for analysis

3. [Backend] Azure OpenAI analyzes story to find relevant text
   const hintPrompt = `Find the sentence(s) in this story that help answer: "${questionText}"`
   â†’ Returns: { relevantSentences: ["The adventure taught him..."], confidence: 0.9 }

4. [Backend â†’ Frontend] Returns hint data with text locations

5. [Frontend] Visual highlighting system activates:
   - Scrolls reading container to relevant paragraph
   - Highlights sentences with golden animated border
   - Adds "ðŸ’¡ Hint" badge next to highlighted text
   - Tracks hint usage for learning analytics
```

### ðŸ”Š **User Story 2: Word Pronunciation Learning** (Audio + Phonetics)

**As a** child learning Korean vocabulary
**I want to** click any Korean word to hear how it's pronounced
**So that** I can learn correct pronunciation while reading

#### Frontend/Backend Flow:
```typescript
// User clicks Korean word "ì•ˆë…•í•˜ì„¸ìš”" in the reading passage
1. [Frontend] Detects click on Korean text span
   const wordRequest = {
     word: "ì•ˆë…•í•˜ì„¸ìš”",
     context: "ì•ˆë…•í•˜ì„¸ìš”, ì¹œêµ¬ë“¤!",
     userId: "student123",
     currentTheme: "space"
   }

2. [Frontend] Checks local audio cache first
   if (audioCache.has("ì•ˆë…•í•˜ì„¸ìš”")) {
     playAudio(cached); return;
   }

3. [Frontend â†’ Backend] POST /api/word-pronunciation
   â†’ Requests audio + phonetic data

4. [Backend] Azure Speech Services generates:
   - Audio file: TTS with Korean neural voice
   - Phonetic notation: "annyeonghaseyo"
   - IPA notation: "[ÉnËjÊŒÅ‹ÉsÉ›jo]"
   - Definition: "Hello (formal greeting)"

5. [Backend â†’ Frontend] Returns audio blob + metadata

6. [Frontend] Rich interaction experience:
   - Plays audio pronunciation
   - Shows floating tooltip with phonetics
   - Displays themed visual feedback
   - Caches audio for future clicks
   - Tracks vocabulary engagement
```

### ðŸ“– **User Story 3: "Read to Me" Feature** (Audio Support)

**As a** child who learns better through listening
**I want to** have the story read aloud while following along
**So that** I can improve both listening and reading comprehension

#### Frontend/Backend Flow:
```typescript
// User clicks "Read to Me" button on current paragraph
1. [Frontend] Captures current reading position and user preferences
   const readingRequest = {
     textContent: currentParagraph.blendedText,
     startPosition: userScrollPosition,
     audioSettings: {
       speed: 1.0,
       voice: "korean-neural",
       highlightWords: true
     },
     languageBlendLevel: 3
   }

2. [Frontend â†’ Backend] POST /api/text-to-speech
   â†’ Sends text content for audio generation

3. [Backend] Intelligent audio processing:
   - Detects mixed English/Korean content
   - Applies appropriate voices for each language
   - Generates SSML markup for natural speech
   - Creates word-timing metadata for highlighting

4. [Backend â†’ Frontend] Streams audio + timing data

5. [Frontend] Synchronized reading experience:
   - Begins audio playback
   - Highlights current word/phrase as spoken
   - Provides playback controls (pause, skip, speed)
   - Auto-scrolls to keep highlighted text visible
   - Shows visual progress indicator
   - Allows user to click ahead to any word
```

### ðŸ§  **User Story 4: Smart Quiz Feedback** (Multiple Features)

**As a** student who got a quiz answer wrong
**I want to** understand why my answer was incorrect and learn the right answer
**So that** I can improve my reading comprehension skills

#### Frontend/Backend Flow:
```typescript
// User selects wrong answer on multiple choice question
1. [Frontend] Captures incorrect response and provides multi-modal feedback
   const feedbackRequest = {
     questionId: "q3",
     userAnswer: "Option B",
     correctAnswer: "Option A",
     storyContent: fullStoryText,
     languageSettings: currentLanguageSupport
   }

2. [Frontend â†’ Backend] POST /api/generate-explanation
   â†’ Requests educational feedback

3. [Backend] Comprehensive analysis using 3 systems:
   - Story analysis: Find supporting evidence for correct answer
   - Common mistakes: Identify why wrong answer was tempting
   - Learning opportunity: Generate age-appropriate explanation

4. [Backend â†’ Frontend] Returns rich feedback object

5. [Frontend] Multi-feature educational response:

   Visual Context:
   - Highlights correct passage section
   - Shows comparison between user choice vs. correct answer

   Audio Support:
   - Reads explanation aloud if requested
   - Pronounces key vocabulary words

   Grammar Hints (if enabled):
   - Explains sentence structure that led to answer
   - Shows Korean grammar pattern if relevant
```

### ðŸŽ¨ **User Story 5: Theme-Integrated Language Learning** (Visual Context + Audio)

**As a** child using the "Space" theme
**I want to** language features that match my chosen visual theme
**So that** learning feels immersive and engaging

#### Frontend/Backend Flow:
```typescript
// User has "Space" theme active and clicks vocabulary word
1. [Frontend] Theme-aware interaction design
   const themedRequest = {
     word: "ìš°ì£¼ì„ ", // spaceship in Korean
     currentTheme: "space",
     userGradeLevel: "4th",
     visualPreferences: {
       animations: true,
       soundEffects: true,
       colorScheme: "space"
     }
   }

2. [Frontend â†’ Backend] POST /api/themed-vocabulary
   â†’ Requests themed learning content

3. [Backend] Theme-contextual processing:
   - Generates space-themed example sentences
   - Selects appropriate vocabulary connections
   - Creates context-relevant audio scripts

4. [Frontend] Immersive themed experience:
   - Shows space-styled tooltip with rocket animation
   - Plays audio with subtle space sound effects
   - Uses theme colors for visual highlighting
   - Displays vocabulary in space context: "The ìš°ì£¼ì„  (spaceship) flew to Mars"
   - Maintains educational value while being visually engaging
```

## Technical Architecture

### Frontend Integration Points

**App.tsx Integration**:
```typescript
// Enhanced Language Support State
const [languageSupport, setLanguageSupport] = useState({
  phonetics: {
    enabled: true,
    notation: 'simplified',
    trigger: 'click'
  },
  audioSupport: {
    enabled: false,
    speed: 1.0,
    autoHighlight: true
  },
  visualContext: {
    enabled: true,
    hintMode: true,
    tooltips: true
  },
  romanization: {
    enabled: true,
    system: 'revised',
    fadeOut: false
  },
  grammarHints: {
    enabled: true,
    complexity: 'basic'
  }
});
```

**Component Architecture**:
```
src/components/language-support/
â”œâ”€â”€ AudioPlayer.tsx          # Text-to-speech controls
â”œâ”€â”€ PhoneticsDisplay.tsx     # IPA notation tooltips
â”œâ”€â”€ VisualHighlighter.tsx    # Text highlighting system
â”œâ”€â”€ RomanizationOverlay.tsx  # Romanization display
â”œâ”€â”€ GrammarHintPanel.tsx     # Grammar explanations
â””â”€â”€ LanguageSupportProvider.tsx # Context provider
```

### Backend API Endpoints

```javascript
// New Language Support Endpoints
app.post('/api/generate-phonetics', async (req, res) => {
  // Generate phonetic notation for Korean text
});

app.post('/api/text-to-speech', async (req, res) => {
  // Convert text to speech audio
});

app.post('/api/romanize-text', async (req, res) => {
  // Convert Korean to romanized text
});

app.post('/api/analyze-grammar', async (req, res) => {
  // Analyze grammar patterns in text
});
```

## Success Metrics

### Learning Outcomes
- **Vocabulary Retention**: 25% improvement in vocabulary quiz scores
- **Pronunciation Accuracy**: Measurable through speech recognition
- **Reading Comprehension**: 15% improvement in quiz completion rates
- **Engagement Time**: 30% increase in time spent reading passages

### Usage Analytics
- **Audio Feature Usage**: >60% of users engage with audio support
- **Visual Hints**: Quiz hint usage correlates with learning success
- **Phonetics Engagement**: Track clicks on phonetic notations
- **Accessibility Impact**: Measure usage by diverse learners

## Implementation Roadmap

### Week 1-2: Visual Context Foundation
- Implement text highlighting system
- Add quiz hint functionality
- Create tooltip infrastructure
- Basic visual accessibility features

### Week 3-5: Audio Support Integration
- Azure Speech Services setup
- Text-to-speech for words and sentences
- Audio playback controls
- Visual highlighting during audio

### Week 6-8: Phonetics System
- Korean pronunciation API integration
- IPA notation display
- Click-to-hear functionality
- Phonetic accuracy validation

### Week 9-10: Polish & Testing
- Romanization support (if prioritized)
- User testing and feedback
- Performance optimization
- Accessibility compliance

## Risk Mitigation

### Technical Risks
- **API Rate Limits**: Cache common pronunciations
- **Audio Performance**: Preload frequent vocabulary audio
- **Browser Compatibility**: Fallbacks for older browsers

### Educational Risks
- **Romanization Dependency**: Gradual phase-out mechanism
- **Over-reliance on Hints**: Limit hint usage frequency
- **Audio Distraction**: Optional auto-play settings

## Budget Considerations

### Third-Party Services
- **Azure Speech Services**: ~$50-200/month depending on usage
- **Pronunciation APIs**: ~$25-100/month
- **Additional Storage**: Audio cache requirements

### Development Resources
- **Frontend Developer**: 6-8 weeks
- **Backend Integration**: 3-4 weeks
- **UX/Accessibility Testing**: 2 weeks
- **Educational Content Review**: 1-2 weeks

## Next Steps

1. **Stakeholder Review**: Validate feature priorities with educators
2. **Technical Spike**: Proof-of-concept for Azure Speech integration
3. **User Research**: Interview target age group (3rd-6th graders)
4. **MVP Definition**: Finalize Phase 1 scope and timeline

---

**Document Status**: Draft for Review
**Created**: 2025-09-28
**Next Review**: Pending stakeholder feedback