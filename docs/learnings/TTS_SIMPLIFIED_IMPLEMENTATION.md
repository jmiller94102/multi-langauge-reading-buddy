# TTS Simplified Implementation Plan
**Date:** 2025-10-01
**Status:** ‚úÖ Azure OpenAI TTS confirmed working for both English and Korean
**Architecture:** Single-voice multilingual (Azure OpenAI TTS only)

---

## üéâ **BREAKTHROUGH: Testing Results**

**User confirmed:** All Azure OpenAI voices (Nova, Shimmer, Fable, etc.) work perfectly for:
- ‚úÖ Pure English text
- ‚úÖ Pure Korean text
- ‚úÖ Mixed English+Korean text (language switching mid-sentence)

**Impact:** We can eliminate the complex dual-service architecture and use Azure OpenAI TTS for everything!

---

## üöÄ **SIMPLIFIED ARCHITECTURE**

### **Before (Complex):**
```
User text ‚Üí Language detection ‚Üí Route to Azure Speech (Korean) OR Azure OpenAI (English)
                                ‚Üì                              ‚Üì
                        ko-KR-SunHiNeural              10 English voices
                                ‚Üì                              ‚Üì
                        Mixed text requires splitting ‚Üí Sequential playback
```

**Problems:**
- Voice mismatch (browser voice names vs Azure IDs)
- Mixed language routing undefined
- Complex text splitting required
- Cache doesn't include voice
- 10 design flaws

---

### **After (Simple):**
```
User text ‚Üí Azure OpenAI TTS (any of 10 voices)
                    ‚Üì
         Handles English, Korean, or Mixed automatically
                    ‚Üì
            Single audio file returned
```

**Benefits:**
- ‚úÖ All 10 voices work for both languages
- ‚úÖ No text splitting needed
- ‚úÖ No language routing logic
- ‚úÖ Voice changes work perfectly
- ‚úÖ Blend levels work seamlessly
- ‚úÖ Simpler cache (text + voice)
- ‚úÖ Fixes 8 out of 10 design flaws automatically

---

## üìã **IMPLEMENTATION PLAN (Revised)**

### **Phase 1: Backend Simplification** (30 minutes)

#### **Change 1.1: Remove Language Routing**

**File:** `backend/server.js` (lines 1154-1194)

**Current Code:**
```javascript
// Determine if we need Korean TTS (use Azure Speech) or English TTS (use Azure OpenAI)
const isKorean = language === 'korean' || language === 'ko-KR' ||
                 /[\u3131-\u3163\uac00-\ud7a3]/.test(text);

if (isKorean) {
  console.log('üá∞üá∑ Detected Korean content - using Azure Speech Services');
  return await generateKoreanTTS(text, speed, res);
} else {
  console.log('üá∫üá∏ Detected English content - using Azure OpenAI TTS');
  return await generateEnglishTTS(text, voice, speed, res);
}
```

**New Code:**
```javascript
// Use Azure OpenAI TTS for ALL languages (supports English + Korean + mixed)
console.log('üåê Using Azure OpenAI TTS (multilingual support)');
return await generateEnglishTTS(text, voice, speed, res);
```

**Impact:** Eliminates routing complexity, works for all blend levels

---

#### **Change 1.2: Rename Function for Clarity**

**Current:** `generateEnglishTTS()` (misleading name)
**New:** `generateAzureOpenAITTS()`

**File:** `backend/server.js` (line 1267)

```javascript
// Rename function to reflect multilingual capability
async function generateAzureOpenAITTS(text, voice, speed, res) {
  try {
    const apiKey = process.env.AZURE_OPENAI_API_KEY;
    const ttsServiceBase = process.env.TTS_SERVICE_API_BASE;
    const ttsDeployment = 'gpt-4o-mini-tts-2';

    if (!apiKey || !ttsServiceBase) {
      console.error('‚ùå Azure OpenAI TTS credentials not found');
      return res.status(500).json({ error: 'TTS service not configured' });
    }

    const availableVoices = [
      'alloy', 'ash', 'ballad', 'coral', 'echo',
      'fable', 'nova', 'onyx', 'sage', 'shimmer'
    ];

    const selectedVoice = voice && availableVoices.includes(voice.toLowerCase())
      ? voice.toLowerCase()
      : 'nova';

    console.log('üé§ Using Azure OpenAI TTS voice:', selectedVoice);
    console.log('üéµ Generating audio for text:', text.substring(0, 100) + '...');

    const ttsUrl = `${ttsServiceBase}/openai/deployments/${ttsDeployment}/audio/speech?api-version=2025-03-01-preview`;
    const fetch = (await import('node-fetch')).default;

    const response = await fetch(ttsUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify({
        model: ttsDeployment,
        input: text,
        voice: selectedVoice,
        speed: speed || 1.0
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('‚ùå Azure OpenAI TTS API error:', response.status, errorText);
      return res.status(response.status).json({
        error: 'TTS generation failed',
        details: errorText
      });
    }

    const audioBuffer = await response.buffer();
    const audioBase64 = audioBuffer.toString('base64');

    console.log('‚úÖ Multilingual audio generated successfully');
    console.log('üìä Audio size:', audioBuffer.length, 'bytes');

    res.json({
      success: true,
      audio: `data:audio/mp3;base64,${audioBase64}`,
      duration: audioBuffer.length,
      childSafe: true,
      voice: selectedVoice,
      language: 'multilingual', // ‚úÖ NEW: Indicates supports both languages
      provider: 'Azure OpenAI TTS',
      availableVoices: availableVoices
    });

  } catch (error) {
    console.error('‚ùå Azure OpenAI TTS generation failed:', error);
    res.status(500).json({
      error: 'TTS generation failed',
      details: error.message
    });
  }
}
```

---

#### **Change 1.3: Keep Azure Speech as Fallback Only** (Optional)

We can keep `generateKoreanTTS()` as a fallback if Azure OpenAI is unavailable, but it's no longer the primary path.

---

### **Phase 2: Frontend Simplification** (45 minutes)

#### **Change 2.1: Fix Cache Key to Include Voice**

**File:** `reading_webapp/src/services/AudioCacheService.ts` (line 53)

**Current:**
```typescript
private generateKey(text: string, language: string): string {
  return `${text.trim().toLowerCase()}-${language}`;
}
```

**New:**
```typescript
private generateKey(text: string, language: string, voice?: string): string {
  const voicePart = voice ? `-${voice.toLowerCase()}` : '';
  return `${text.trim().toLowerCase()}-${language}${voicePart}`;
}
```

**Update all callers:**

```typescript
// Line 142: get method
public get(text: string, language: string, voice?: string): string | null {
  const key = this.generateKey(text, language, voice);
  // ... rest unchanged
}

// Line 165: set method
public set(text: string, language: string, voice: string, audioData: string): void {
  const key = this.generateKey(text, language, voice);
  // ... rest unchanged
}

// Line 221: preload method signature
public async preload(
  texts: string[],
  language: string,
  voice: string, // ‚úÖ NEW parameter
  onProgress?: (loaded: number, total: number) => void
): Promise<void> {
  const uncachedTexts = texts.filter(text => !this.get(text, language, voice));
  // ... update fetch body to include voice parameter
}
```

**Impact:** ‚úÖ Fixes Flaw #1 - Voice changes now work correctly

---

#### **Change 2.2: Update ProfessionalAudioPlayer to Pass Voice to Cache**

**File:** `reading_webapp/src/components/language-support/ProfessionalAudioPlayer.tsx`

**Update loadAudio function (lines 166-220):**

```typescript
const loadAudio = useCallback(async (forceRefresh = false) => {
  if (!enabled || !text.trim()) return;

  try {
    setIsLoading(true);
    setError(null);

    // Get selected voice name
    const selectedVoice = availableVoices[selectedVoiceIndex];
    const voiceName = selectedVoice?.name.toLowerCase() || 'nova';

    // Check cache first (now includes voice in key)
    if (!forceRefresh) {
      const cached = cacheService.get(text, language, voiceName);
      if (cached && audioRef.current) {
        console.log('üîÅ Using cached audio for voice:', voiceName);
        audioRef.current.src = cached;
        setIsLoading(false);
        return;
      }
    }

    // Backend TTS (now handles all languages with same voice)
    try {
      const response = await fetch('http://localhost:3001/api/text-to-speech', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          text: cleanTextForTTS(text),
          voice: voiceName, // ‚úÖ Send voice name directly (no mapping needed!)
          speed: playbackSpeed,
          childSafe
        })
      });

      if (response.ok) {
        const result = await response.json();
        if (result.success && result.audio && audioRef.current) {
          // Cache with voice included in key
          cacheService.set(text, language, voiceName, result.audio);
          audioRef.current.src = result.audio;
          console.log('‚úÖ Audio loaded with voice:', voiceName);
          setIsLoading(false);
          return;
        }
      }
    } catch (backendError) {
      console.log('üéµ Backend TTS unavailable, using browser TTS fallback');
    }

    // Browser TTS fallback (unchanged)
    setIsLoading(false);

  } catch (error) {
    console.error('üéµ Audio loading failed:', error);
    setIsLoading(false);
  }
}, [text, language, childSafe, enabled, playbackSpeed, availableVoices, selectedVoiceIndex]);
```

**Impact:** ‚úÖ Fixes Flaw #1 - Cache now respects voice changes

---

#### **Change 2.3: Remove Language Parameter (No Longer Needed)**

Since Azure OpenAI TTS handles both languages automatically, we can simplify the API contract:

**Before:**
```typescript
body: JSON.stringify({
  text: cleanTextForTTS(text),
  language: language === 'korean' ? 'korean' : 'english', // ‚ùå Not needed
  voice: voiceName,
  speed: playbackSpeed,
  childSafe
})
```

**After:**
```typescript
body: JSON.stringify({
  text: cleanTextForTTS(text),
  voice: voiceName,  // Azure OpenAI auto-detects language from text
  speed: playbackSpeed,
  childSafe
})
```

---

#### **Change 2.4: Simplify Voice Selection (No Browser/Azure Mapping Needed)**

**Remove complexity:**
- ‚ùå No need for `VoiceMappingService.ts` (we were going to create this)
- ‚ùå No need to map browser voices to Azure voices
- ‚úÖ User selects from browser voices ‚Üí Frontend uses browser TTS
- ‚úÖ Backend always uses Azure OpenAI TTS with best-match voice

**Keep it simple:**
- Frontend sends voice name as-is: `"Samantha"`, `"nova"`, `"shimmer"`, etc.
- Backend validates against allowed list and uses closest match

**Update backend validation:**
```javascript
const availableVoices = ['alloy', 'ash', 'ballad', 'coral', 'echo', 'fable', 'nova', 'onyx', 'sage', 'shimmer'];

// Smart matching: handle browser voice names gracefully
const selectedVoice = (() => {
  if (!voice) return 'nova'; // Default

  const lowerVoice = voice.toLowerCase();

  // Exact match
  if (availableVoices.includes(lowerVoice)) {
    return lowerVoice;
  }

  // Partial match (e.g., "Samantha (English)" ‚Üí match "sam" pattern)
  const partialMatch = availableVoices.find(v => lowerVoice.includes(v) || v.includes(lowerVoice));
  if (partialMatch) {
    console.log(`üéµ Mapped "${voice}" to "${partialMatch}"`);
    return partialMatch;
  }

  // Fallback to default
  console.log(`üéµ Unknown voice "${voice}", using default "nova"`);
  return 'nova';
})();
```

**Impact:** ‚úÖ Fixes Flaw #3 - Backend now accepts voice selection

---

### **Phase 3: Auto-Update on Blend Level Change** (30 minutes)

#### **Change 3.1: Pre-load Audio When Text Changes**

**File:** `reading_webapp/src/components/language-support/ProfessionalAudioPlayer.tsx`

```typescript
// Track previous text to detect blend level changes
const prevTextRef = useRef<string>(text);

useEffect(() => {
  const textChanged = prevTextRef.current !== text;

  if (textChanged && enabled) {
    console.log('üéµ Text changed (blend level adjusted), pre-loading new audio...');

    // Stop current playback
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
    }
    if (currentUtterance) {
      window.speechSynthesis.cancel();
      setCurrentUtterance(null);
    }

    // Reset playback state
    setIsPlaying(false);
    setPlaybackProgress(0);
    setStartTime(null);

    // Pre-load new audio in background
    loadAudio(true); // Force refresh for new text

    prevTextRef.current = text;
  }
}, [text, enabled, loadAudio, currentUtterance]);
```

**Impact:** ‚úÖ Fixes Flaw #8 - Blend level changes auto-load new audio

---

### **Phase 4: Voice Preference Persistence** (15 minutes)

#### **Change 4.1: Save Voice Preference to localStorage**

**Create service:** `reading_webapp/src/services/VoicePreferenceService.ts`

```typescript
export class VoicePreferenceService {
  private static readonly STORAGE_KEY = 'readquest_voice_preference';

  static getPreference(): string | null {
    try {
      return localStorage.getItem(this.STORAGE_KEY);
    } catch {
      return null;
    }
  }

  static setPreference(voiceName: string): void {
    try {
      localStorage.setItem(this.STORAGE_KEY, voiceName);
      console.log('üíæ Saved voice preference:', voiceName);
    } catch (error) {
      console.warn('Failed to save voice preference:', error);
    }
  }
}
```

**Update ProfessionalAudioPlayer.tsx:**

```typescript
import { VoicePreferenceService } from '../../services/VoicePreferenceService';

// On voice load, restore saved preference
useEffect(() => {
  if (availableVoices.length > 0 && selectedVoiceIndex === -1) {
    const savedVoiceName = VoicePreferenceService.getPreference();

    if (savedVoiceName) {
      const savedIndex = availableVoices.findIndex(v =>
        v.name.toLowerCase() === savedVoiceName.toLowerCase()
      );
      if (savedIndex !== -1) {
        console.log('üéµ Restored saved voice:', savedVoiceName);
        setSelectedVoiceIndex(savedIndex);
        return;
      }
    }

    // No saved preference - use best voice
    const bestVoiceIndex = findBestVoiceIndex(availableVoices, language);
    setSelectedVoiceIndex(bestVoiceIndex);
  }
}, [availableVoices, selectedVoiceIndex, language]);

// On voice change, save preference
const handleVoiceChange = async (voiceIndex: number) => {
  // ... existing stop/reset code ...

  const newVoice = availableVoices[voiceIndex];
  setSelectedVoiceIndex(voiceIndex);

  // Save preference
  VoicePreferenceService.setPreference(newVoice.name);

  // Pre-load audio with new voice
  await loadAudio(true);

  // ... rest of code ...
};
```

**Impact:** ‚úÖ Fixes Flaw #7 - Voice preference persists across sessions

---

## üìä **WHAT GETS FIXED AUTOMATICALLY**

By switching to single-voice architecture:

| Flaw | Status | How Fixed |
|------|--------|-----------|
| #1: Cache Missing Voice | ‚úÖ **FIXED** | Updated cache key to include voice |
| #2: Voice Loading Race | ‚ö†Ô∏è **STILL NEEDS FIX** | Promise-based loading still needed |
| #3: Backend/Browser Mismatch | ‚úÖ **FIXED** | No mapping needed, Azure handles all |
| #4: No Audio Pre-generation | ‚úÖ **FIXED** | Added pre-load on voice change |
| #5: Mixed Language Routing | ‚úÖ **FIXED** | Azure OpenAI handles automatically |
| #6: No Retry Logic | ‚ö†Ô∏è **STILL NEEDS FIX** | Add retry with backoff |
| #7: Voice Not Persisted | ‚úÖ **FIXED** | Added localStorage persistence |
| #8: Blend Level Audio Update | ‚úÖ **FIXED** | Added auto-load on text change |
| #9: No Timeout | ‚ö†Ô∏è **STILL NEEDS FIX** | Add AbortController timeout |
| #10: Utterance Queue | ‚ö†Ô∏è **STILL NEEDS FIX** | Better queue management |

**Result:** 5/10 flaws fixed by architecture change, 3/10 need minor fixes, 2/10 still need implementation

---

## üß™ **TESTING PLAN**

### **Manual Test Cases:**

1. **Voice Change Test:**
   ```
   - Play story with Nova voice
   - Pause
   - Change to Shimmer voice
   - Play
   - Verify: Audio uses Shimmer (not cached Nova)
   ```

2. **Blend Level Test:**
   ```
   - Play at blend level 0 (100% English)
   - Move slider to level 5 (50/50 mixed)
   - Verify: Audio auto-loads
   - Click play
   - Verify: Korean words pronounced correctly
   ```

3. **Voice Persistence Test:**
   ```
   - Select Fable voice
   - Generate new story
   - Verify: Fable still selected
   - Refresh page
   - Verify: Fable still selected
   ```

4. **Mixed Language Quality Test:**
   ```
   - Set blend level to 5
   - Generate story with Korean/English mix
   - Play audio
   - Verify: Smooth language transitions
   - Verify: Both languages sound natural
   ```

---

## ‚è±Ô∏è **IMPLEMENTATION TIME ESTIMATE**

- Phase 1 (Backend): **30 minutes**
- Phase 2 (Frontend Cache): **45 minutes**
- Phase 3 (Auto-Update): **30 minutes**
- Phase 4 (Persistence): **15 minutes**
- Testing & Debugging: **60 minutes**

**Total: ~3 hours** (vs 8+ hours for complex architecture)

---

## üöÄ **IMPLEMENTATION ORDER**

```
1. ‚úÖ Backend: Remove language routing (server.js:1176-1184)
2. ‚úÖ Backend: Rename function to generateAzureOpenAITTS
3. ‚úÖ Frontend: Update AudioCacheService.ts (add voice to key)
4. ‚úÖ Test: Voice change with cache invalidation
5. ‚úÖ Frontend: Update ProfessionalAudioPlayer (pass voice to cache)
6. ‚úÖ Test: Voice selection end-to-end
7. ‚úÖ Frontend: Add auto-load on text change
8. ‚úÖ Test: Blend level changes
9. ‚úÖ Frontend: Add voice persistence
10. ‚úÖ Test: Full user flow (select voice ‚Üí change blend ‚Üí refresh page)
```

---

## ‚ùì **REMAINING DECISIONS**

### **1. Keep Azure Speech Services as Fallback?**

**Option A:** Remove entirely (simpler)
- Pro: Less code, fewer dependencies
- Con: No fallback if Azure OpenAI has issues

**Option B:** Keep as fallback (safer)
- Pro: Graceful degradation
- Con: More code to maintain

**Recommendation:** Keep as fallback, but make it explicit "backup" mode

---

### **2. Browser TTS Fallback**

Currently falls back to browser TTS if backend unavailable. Keep this?

**Recommendation:** YES - Good for offline/development scenarios

---

### **3. Voice Auto-Play on Change**

When user changes voice, should we:
- **Option A:** Pre-load only, user clicks play
- **Option B:** Auto-play with new voice

**Recommendation:** Option A (pre-load only) - less surprising UX

---

## üéØ **READY TO IMPLEMENT?**

All research complete. Simplified architecture validated. Implementation plan ready.

**Do you want me to:**
1. **Start implementing now** (following the order above)
2. **Answer remaining questions first** (fallback strategy, auto-play behavior)
3. **Review the plan with you** before starting

Let me know and I'll begin! üöÄ
